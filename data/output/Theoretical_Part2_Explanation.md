# Theoretical Part 2: Mathematical and Algorithmic Explanation

## A. Enrichment Technique

### Algorithmic Explanation
1. **Data Extraction:**  
   - **Fetch External Data:** Use public APIs or datasets (e.g., Countries API, MovieLens, etc.) to obtain additional features.
   - **Preprocessing:** Clean the data by removing duplicates, standardizing formats, and handling missing values.

2. **Data Integration (Joining):**  
   - **Key Matching:** Identify a common key (such as a country code or unique identifier) that exists in both your original dataset and the external data.
   - **Join Operation:** Merge the two datasets using a join operation (e.g., a natural join or left join).  
     - **Algorithmically:** A common approach is a hash join. Build a hash table on the join key for one dataset and iterate through the second dataset to match records.
     - **Complexity:** On average, this operation runs in approximately \( O(n) \) time if hash look-ups are constant time.

### Mathematical Explanation
- **Representation:**  
  Represent the original dataset as a matrix \( A \in \mathbb{R}^{n \times m} \) (with \( n \) records and \( m \) features) and the external dataset as \( B \in \mathbb{R}^{n \times p} \) (assuming both are aligned by the key).
  
- **Enriched Dataset:**  
  The enriched dataset \( C \) is formed by concatenating the features:
  \[
  C = [A \;|\; B] \quad \text{where } C \in \mathbb{R}^{n \times (m+p)}
  \]
  This is analogous to the natural join in relational algebra, where rows with matching keys are merged.

---

## B. Dimensionality Reduction Approach: t-SNE

### Algorithmic Explanation

1. **Input Preparation:**  
   - **Feature Scaling:** Scale the high-dimensional data so that each feature contributes equally.
   - **Similarity Computation:** Compute pairwise similarities between high-dimensional data points using a Gaussian kernel.

2. **t-SNE Process:**  
   - **High-Dimensional Similarity:**  
     For each pair of points \( x_i \) and \( x_j \), define the conditional probability:
     \[
     p_{j|i} = \frac{\exp\left(-\frac{\|x_i - x_j\|^2}{2\sigma_i^2}\right)}{\sum_{k \neq i} \exp\left(-\frac{\|x_i - x_k\|^2}{2\sigma_i^2}\right)}
     \]
     Symmetrize it as:
     \[
     p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}
     \]

   - **Low-Dimensional Mapping:**  
     Map each high-dimensional point \( x_i \) to a low-dimensional point \( y_i \) (typically in 2D). In this space, define a similarity measure using a Student's t-distribution:
     \[
     q_{ij} = \frac{\left(1 + \|y_i - y_j\|^2\right)^{-1}}{\sum_{k \neq l} \left(1 + \|y_k - y_l\|^2\right)^{-1}}
     \]

   - **Cost Function:**  
     The objective is to minimize the Kullback-Leibler divergence between the high-dimensional and low-dimensional similarity distributions:
     \[
     KL(P\|Q) = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}
     \]

   - **Optimization:**  
     Use gradient descent to iteratively update the positions \( y_i \), reducing the KL divergence so that similar points in the original space remain close in the low-dimensional mapping.

### Mathematical Summary

- **High-Dimensional Similarity:**  
  \( p_{ij} \) quantifies the probability that \( x_i \) would select \( x_j \) as its neighbor.

- **Low-Dimensional Similarity:**  
  \( q_{ij} \) uses a heavy-tailed Studentâ€™s t-distribution to model the similarities in the low-dimensional space, helping to preserve moderate distances.

- **Objective:**  
  The goal is to minimize \( KL(P\|Q) \) to ensure that the low-dimensional embedding \( \{y_i\} \) accurately reflects the neighborhood structure of the original high-dimensional dataset.
